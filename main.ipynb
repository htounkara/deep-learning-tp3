{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import zipfile\n",
    "\n",
    "# Const \n",
    "\n",
    "sequence_length = 18000\n",
    "n_test = 5000\n",
    "zip_file_path = './data/training2017.zip'\n",
    "\n",
    "# Utils\n",
    "\n",
    "def load_ecg(zip_file, file_name):\n",
    "    with zip_file.open(file_name) as mat_file:\n",
    "        mat = scipy.io.loadmat(mat_file)\n",
    "        ecg_data = mat['val'].squeeze()\n",
    "    return ecg_data\n",
    "\n",
    "def standardize_series(series):\n",
    "    mean = np.mean(series, axis=1, keepdims=True)\n",
    "    std = np.std(series, axis=1, keepdims=True)\n",
    "    return (series - mean) / std\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def normalize_ecg(ecg_data):\n",
    "    ecg_data = scaler.fit_transform(ecg_data.reshape(-1, 1)).flatten()\n",
    "    return ecg_data\n",
    "\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "def normalize_ecg_minmax(ecg_data):\n",
    "    ecg_data = scaler2.fit_transform(ecg_data.reshape(-1, 1)).flatten()\n",
    "    return ecg_data\n",
    "\n",
    "def pad_or_trim_ecg(ecg_data, target_length):\n",
    "    if len(ecg_data) > target_length:\n",
    "        return ecg_data[:target_length]\n",
    "    elif len(ecg_data) < target_length:\n",
    "        return np.pad(ecg_data, (0, target_length - len(ecg_data)), 'constant')\n",
    "    return ecg_data\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, ecg_data, labels):\n",
    "        self.ecg_data = ecg_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecg_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ecg = self.ecg_data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(ecg, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class ECGCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=7, stride=1, padding=3)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 281, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)  # 4 classes de sortie\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool1d(4)\n",
    "\n",
    "    def forward(self, x):\n",
    "       x = x.unsqueeze(1)\n",
    "       x = self.pool(F.relu(self.conv1(x)))\n",
    "       x = self.pool(F.relu(self.conv2(x)))\n",
    "       x = self.pool(F.relu(self.conv3(x)))\n",
    "       \n",
    "       x = x.view(x.size(0), -1)\n",
    "       x = F.relu(self.fc1(x))\n",
    "       x = self.fc2(x)\n",
    "       return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "df_labels = pd.read_csv('./data/REFERENCE.csv', header=None, names=['file', 'label'])\n",
    "label_mapping = {'N': 0, 'A': 1, 'O': 2, '~': 3}\n",
    "df_labels['label'] = df_labels['label'].map(label_mapping)\n",
    "\n",
    "ecg_files = df_labels['file'].values\n",
    "ecg_data_list = []\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "    for k in range(n_test):\n",
    "        file_name = f'training2017/{ecg_files[k]}.mat' \n",
    "        ecg_data = load_ecg(zip_file, file_name)\n",
    "        ecg_data_list.append(ecg_data)\n",
    "\n",
    "assert(len(ecg_data_list) == n_test)\n",
    "\n",
    "ecg_data_list = [normalize_ecg_minmax(pad_or_trim_ecg(ecg, sequence_length)) for ecg in ecg_data_list]\n",
    "\n",
    "labels = df_labels['label'].values[:n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.0076143369674682\n",
      "Epoch 2/5, Loss: 0.9856883358955383\n",
      "Epoch 3/5, Loss: 0.9658912172317505\n",
      "Epoch 4/5, Loss: 0.9400389642715454\n",
      "Epoch 5/5, Loss: 0.9036868848800659\n"
     ]
    }
   ],
   "source": [
    "# Training CNN\n",
    "\n",
    "dataset = ECGDataset(ecg_data_list, labels)\n",
    "ecg_train, ecg_test, labels_train, labels_test = train_test_split(ecg_data_list, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ECGDataset(ecg_train, labels_train)\n",
    "test_dataset = ECGDataset(ecg_test, labels_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = ECGCNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {running_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.40%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation CNN\n",
    "\n",
    "model.eval() \n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# On désactive le calcul du gradient pdt le test\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.1590149904054308\n",
      "Epoch 2/5, Loss: 0.9960977633794149\n",
      "Epoch 3/5, Loss: 0.999531868904356\n",
      "Epoch 4/5, Loss: 0.9923074046770731\n",
      "Epoch 5/5, Loss: 0.9916038248274062\n"
     ]
    }
   ],
   "source": [
    "labels_rnn = df_labels['label'].values[:n_test]\n",
    "factor=4\n",
    "\n",
    "def downsample_ecg_rnn(ecg_data):\n",
    "    return ecg_data[::factor]\n",
    "\n",
    "ecg_data_list_rnn = [downsample_ecg_rnn(ecg) for ecg in ecg_data_list]\n",
    "sequence_length_rnn = sequence_length // factor\n",
    "\n",
    "ecg_data_list_rnn = [pad_or_trim_ecg(ecg, sequence_length_rnn) for ecg in ecg_data_list_rnn]\n",
    "ecg_data_list_rnn = [normalize_ecg_minmax(ecg) for ecg in ecg_data_list_rnn]\n",
    "\n",
    "ecg_train_rnn, ecg_test_rnn, labels_train_rnn, labels_test_rnn = train_test_split(\n",
    "    ecg_data_list_rnn, labels_rnn, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "batch_size_rnn = 64\n",
    "\n",
    "# Create datasets and data loaders for RNN\n",
    "train_dataset_rnn = ECGDataset(ecg_train_rnn, labels_train_rnn)\n",
    "test_dataset_rnn = ECGDataset(ecg_test_rnn, labels_test_rnn)\n",
    "\n",
    "train_loader_rnn = DataLoader(train_dataset_rnn, batch_size=batch_size_rnn, shuffle=True)\n",
    "test_loader_rnn = DataLoader(test_dataset_rnn, batch_size=batch_size_rnn, shuffle=False)\n",
    "\n",
    "\n",
    "class ECGRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGRNN, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=1, hidden_size=32, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32, 32) \n",
    "        self.fc2 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        gru_out, hn = self.gru(x)\n",
    "        \n",
    "        out = gru_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Training RNN\n",
    "\n",
    "modelRNN = ECGRNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelRNN.parameters(), lr=0.001)\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels_rnn) in enumerate(train_loader_rnn):\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = modelRNN(inputs)\n",
    "        loss = criterion(outputs, labels_rnn)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {running_loss / len(train_loader_rnn)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.40%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation RNN\n",
    "\n",
    "modelRNN.eval() \n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# On désactive le calcul du gradient pendant la phase de test\n",
    "with torch.no_grad():\n",
    "    for inputs, labels_rnn in test_loader_rnn:\n",
    "        outputs = modelRNN(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels_rnn.size(0)\n",
    "        correct += (predicted == labels_rnn).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP1_2IA_ML_ADVANCED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
